{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2e7909-e8d2-4e6d-9938-f278af5968ff",
    "_uuid": "0549a974-10f7-433e-8024-f55c5c4f89a3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# 1. Define Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a11ab3f3-515d-4bfe-a302-33d77a197bb3",
    "_uuid": "3c3d957f-84f2-4cd1-bca0-124b157796cc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-19T09:59:51.312253Z",
     "iopub.status.busy": "2025-12-19T09:59:51.311916Z",
     "iopub.status.idle": "2025-12-19T09:59:51.324191Z",
     "shell.execute_reply": "2025-12-19T09:59:51.323284Z",
     "shell.execute_reply.started": "2025-12-19T09:59:51.312214Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Core Python libraries\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data manipulation and visualization\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "\n",
    "# Machine Learning models and preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "# Machine Learning classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# External libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tempfile\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, jaccard_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, roc_curve\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# External libraries\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7347b68c-f5a3-45ef-81f4-ec04a3f5242a",
    "_uuid": "459828a0-423d-4437-a9a2-58cb39d62fcf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-19T09:59:51.325807Z",
     "iopub.status.busy": "2025-12-19T09:59:51.325499Z",
     "iopub.status.idle": "2025-12-19T09:59:51.347797Z",
     "shell.execute_reply": "2025-12-19T09:59:51.346729Z",
     "shell.execute_reply.started": "2025-12-19T09:59:51.325779Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid', palette='pastel')\n",
    "color = sns.color_palette(palette='pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_export(\n",
    "    classifiers: dict,\n",
    "    train_features, train_labels,\n",
    "    test_features, test_labels,\n",
    "    history_dict: dict = None,\n",
    "    word_file: str = \"results.docx\"\n",
    "):\n",
    "    # Prepare Word documenat\n",
    "    doc =  Document()  #Document(word_file) if os.path.exists(word_file) else Document()\n",
    "\n",
    "    # If labels are one-hot/multilabel, convert them to integer labels\n",
    "    if isinstance(test_labels, np.ndarray) and test_labels.ndim > 1:\n",
    "        test_true = np.argmax(test_labels, axis=1)\n",
    "    else:\n",
    "        test_true = test_labels\n",
    "    \n",
    "\n",
    "    unique_labels = np.unique(test_true)\n",
    "    num_classes = len(unique_labels)\n",
    "    accuracies = []\n",
    "    tmp_files = []\n",
    "    for name, model in classifiers.items():\n",
    "        print(name)\n",
    "        \n",
    "        roc_data = []\n",
    "        # Train\n",
    "        fitted = model.fit(train_features, train_labels)\n",
    "        # Predict\n",
    "        y_pred_raw = model.predict(test_features)\n",
    "\n",
    "        # If predict returns one-hot/multilabel, convert to class indices\n",
    "        if isinstance(y_pred_raw, np.ndarray) and y_pred_raw.ndim > 1:\n",
    "            y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "        else:\n",
    "            y_pred = y_pred_raw\n",
    "        \n",
    "        # Try to get probabilities\n",
    "        try:\n",
    "            y_proba = model.predict_proba(test_features)\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(test_true, y_pred)\n",
    "\n",
    "        # Specificity per class\n",
    "        specs = []\n",
    "        for i in range(cm.shape[0]):\n",
    "            tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "            fp = cm[:,i].sum() - cm[i,i]\n",
    "            specs.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        avg_spec = np.mean(specs)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(test_true, y_pred),\n",
    "            \"Precision\": precision_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Recall\": recall_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"F1 Score\": f1_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Jaccard Index\": jaccard_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Matthews CorrCoef\": matthews_corrcoef(test_true, y_pred),\n",
    "            \"Cohen’s Kappa\": cohen_kappa_score(test_true, y_pred),\n",
    "            \"Specificity\": avg_spec,\n",
    "        }\n",
    "\n",
    "        # AUC\n",
    "        auc = np.nan\n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                if num_classes == 2:\n",
    "                    # assume probabilities are shape (n_samples,2)\n",
    "                    auc = roc_auc_score(test_true, y_proba[:,1])\n",
    "                    fpr, tpr, _ = roc_curve(test_true, y_proba[:,1])\n",
    "                    roc_data.append((name, fpr, tpr, auc))\n",
    "                else:\n",
    "                    # Multi-class: One-vs-Rest ROC for each class\n",
    "                    y_bin = label_binarize(test_true, classes=unique_labels)\n",
    "                    for i, cls in enumerate(unique_labels):\n",
    "                        fpr, tpr, _ = roc_curve(y_bin[:,i], y_proba[:,i])\n",
    "                        auc = roc_auc_score(y_bin[:,i], y_proba[:,i])\n",
    "                        roc_data.append((f\"Class {cls}\", fpr, tpr, auc))\n",
    "            except Exception:\n",
    "                pass\n",
    "        metrics[\"AUC\"] = auc\n",
    "\n",
    "        accuracies.append((name, metrics[\"Accuracy\"] * 100))\n",
    "\n",
    "        # Write to Word\n",
    "        doc.add_heading(f\"Evaluation: {name}\", level=1)\n",
    "        for m, v in metrics.items():\n",
    "            doc.add_paragraph(f\"{m}: {v if not np.isnan(v) else 'N/A'}\")\n",
    "\n",
    "        # Confusion matrix plot\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "            plt.title(name)\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"True\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(tmp.name, dpi=300)\n",
    "            plt.close()\n",
    "            doc.add_picture(tmp.name, width=Inches(4))\n",
    "            tmp_files.append(tmp.name)  # store path for later deletion\n",
    "\n",
    "        # Loss plot if available\n",
    "        if history_dict and name in history_dict:\n",
    "            hist = history_dict[name].history\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "                plt.figure(figsize=(6,6))\n",
    "                plt.plot(hist['loss'], label='Train Loss')\n",
    "                if 'val_loss' in hist:\n",
    "                    plt.plot(hist['val_loss'], label='Val Loss')\n",
    "                plt.title(f\"Loss: {name}\")\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(tmp.name, dpi=300)\n",
    "                plt.close()\n",
    "                doc.add_picture(tmp.name, width=Inches(6))\n",
    "                tmp_files.append(tmp.name)  # store path for later deletion\n",
    "\n",
    "        # Combined ROC for binary classifiers\n",
    "        if roc_data:\n",
    "            doc.add_heading(\"Combined ROC Curves\", level=1)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "                plt.figure(figsize=(6,6))\n",
    "                for label, fpr, tpr, auc_score in roc_data:\n",
    "                    plt.plot(fpr, tpr, label=f\"{label} (AUC={auc_score:.2f})\")\n",
    "                plt.plot([0,1],[0,1], linestyle='--', color='black')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('ROC Curves')\n",
    "                plt.legend(fontsize=8)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(tmp.name, dpi=300)\n",
    "                plt.close()\n",
    "                doc.add_picture(tmp.name, width=Inches(6))\n",
    "                tmp_files.append(tmp.name)  # store path for later deletion\n",
    "        print(metrics[\"Accuracy\"])\n",
    "    doc.save(word_file)\n",
    "    # Delete temp files safely\n",
    "    for f in tmp_files:\n",
    "        os.unlink(f)\n",
    "    print(f\"✅ Results saved to {word_file}\")\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../aug_manifest_filled.csv\"          # <- change\n",
    "COL_ORIGINAL = \"origin\"      # <- change if needed\n",
    "COL_LABEL = \"disease_name\"     # <- change if needed\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "df = pd.read_csv(CSV_PATH).copy()\n",
    "\n",
    "# Keep only unique originals (drop duplicate rows by original)\n",
    "df_unique = df.drop_duplicates(subset=[COL_ORIGINAL], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# 5-fold on unique originals, stratified by disease_name\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "df_unique[\"fold\"] = -1\n",
    "\n",
    "X_dummy = df_unique[[COL_ORIGINAL]]   # placeholder\n",
    "y = df_unique[COL_LABEL]\n",
    "\n",
    "for fold_id, (_, val_idx) in enumerate(skf.split(X_dummy, y), start=1):\n",
    "    df_unique.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "# Check: this should be ~7909 per fold if total unique originals ~39545\n",
    "print(\"Unique originals:\", len(df_unique))\n",
    "print(df_unique[\"fold\"].value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "et_base = ExtraTreesClassifier(\n",
    "    n_estimators=800,          # more trees for stability\n",
    "    max_depth=None,           # let trees grow full\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",      # random subspace often helps with lots of features\n",
    "    random_state=42\n",
    ")\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=800,          # more trees for stability\n",
    "    max_depth=None,           # let trees grow full\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",      # random subspace often helps with lots of features\n",
    "    random_state=42\n",
    ")\n",
    "OvR_classifiers = {\n",
    "    \"ET-OvR\": OneVsRestClassifier(et_base, n_jobs=-1),\n",
    "    \"RF-OvR\": OneVsRestClassifier(rf_base, n_jobs=-1)\n",
    "}\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42,class_weight=\"balanced\"),\n",
    "    \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "410948d5-b7ac-4db5-90d0-469974f858f0",
    "_uuid": "0cd96def-fb09-4c24-b0b3-b052769a01b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-19T09:59:51.349924Z",
     "iopub.status.busy": "2025-12-19T09:59:51.349654Z",
     "iopub.status.idle": "2025-12-19T10:00:35.411439Z",
     "shell.execute_reply": "2025-12-19T10:00:35.410617Z",
     "shell.execute_reply.started": "2025-12-19T09:59:51.349903Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Load CSV files\n",
    "# -----------------------------\n",
    "folds_df = pd.read_csv(\"../folds.csv\")\n",
    "aug_df = pd.read_csv(\"../aug_manifest_filled.csv\")\n",
    "features_df = pd.read_csv(\"../glcm_features_fold1.csv\") \n",
    "\n",
    "features_df[\"path\"] = \"BreaKHis_v1/\" + features_df[\"path\"].astype(str).str.replace(\"\\\\\", \"/\", regex=False).str.split(\"BreaKHis_v1/\").str[-1]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Select fold\n",
    "# -----------------------------\n",
    "\n",
    "for SELECTED_FOLD in [1,2,3,4,5]:\n",
    "    test_files  = df_unique.loc[df_unique[\"fold\"] == SELECTED_FOLD, \"origin\"].unique()\n",
    "    train_files = df_unique.loc[df_unique[\"fold\"] != SELECTED_FOLD, \"origin\"].unique()\n",
    "    train_meta = aug_df[\n",
    "        aug_df[\"origin\"].isin(train_files)\n",
    "    ].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Testing set (clean samples only)\n",
    "    # -----------------------------\n",
    "    test_meta = aug_df[\n",
    "        (aug_df[\"origin\"].isin(test_files)) &\n",
    "        (aug_df[\"crop\"] == \"center\") &\n",
    "        (aug_df[\"flip\"] == \"none\") &\n",
    "        (aug_df[\"rotation\"] == 0)\n",
    "    ].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Merge with features\n",
    "    # -----------------------------\n",
    "    train_df = aug_df.merge(\n",
    "        features_df,\n",
    "        left_on=\"filename\",\n",
    "        right_on=\"path\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    test_df = test_meta.merge(\n",
    "        features_df,\n",
    "        left_on=\"filename\",\n",
    "        right_on=\"path\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Define labels\n",
    "    # -----------------------------\n",
    "    y_train = train_df[\"disease_name\"]\n",
    "    y_test = test_df[\"disease_name\"]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Drop non-feature columns\n",
    "    # -----------------------------\n",
    "    NON_FEATURE_COLS = {\n",
    "        \"filename\", \"path\", \"origin\",\n",
    "        \"crop\", \"flip\", \"rotation\",\n",
    "        \"disease_name\", \"disease_type\",\n",
    "        \"mag\", \"fold\"\n",
    "    }\n",
    "\n",
    "    X_train = train_df.drop(columns=[c for c in NON_FEATURE_COLS if c in train_df.columns])\n",
    "    X_test = test_df.drop(columns=[c for c in NON_FEATURE_COLS if c in test_df.columns])\n",
    "    X_train_path = train_df[\"path\"]\n",
    "    X_test_path = test_df[\"path\"]\n",
    "    # -----------------------------\n",
    "    # Final sanity checks\n",
    "    # -----------------------------\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_test) == len(y_test)\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape, \"Labels:\", y_train.nunique())\n",
    "    print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # =========================================\n",
    "    # 4) STRATIFIED 50:50 SPLIT FOR TRAIN AND TEST\n",
    "    # =========================================\n",
    "\n",
    "    sss_train = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    sss_test  = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "    # ---- Stratified 50:50 split for TRAIN ----\n",
    "    for idx1, idx2 in sss_train.split(X_train, y_train):\n",
    "        train_features  = X_train.iloc[idx1]\n",
    "        train_labels    = y_train.iloc[idx1]\n",
    "        train_path      = X_train_path.iloc[idx1]\n",
    "\n",
    "        train2_features = X_train.iloc[idx2]\n",
    "        train2_labels   = y_train.iloc[idx2]\n",
    "        train_path2      = X_train_path.iloc[idx2]\n",
    "\n",
    "    # ---- Stratified 50:50 split for TEST ----\n",
    "    for idx1, idx2 in sss_test.split(X_test, y_test):\n",
    "        test_features   = X_test.iloc[idx1]\n",
    "        test_labels     = y_test.iloc[idx1]\n",
    "        test_path       = X_test_path[idx1]\n",
    "\n",
    "        test2_features  = X_test.iloc[idx2]\n",
    "        test2_labels    = y_test.iloc[idx2]\n",
    "        test_path2      = X_test_path[idx2]\n",
    "\n",
    "\n",
    "    print(\"train_features:\", train_features.shape, \"train2_features:\", train2_features.shape)\n",
    "    print(\"test_features:\",  test_features.shape,  \"test2_features:\",  test2_features.shape)\n",
    "\n",
    "\n",
    "    # =========================================\n",
    "    # 5) ENCODE LABELS INTO NUMERICAL FORM (FOR ML MODELS)\n",
    "    # =========================================\n",
    "\n",
    "    # Fit encoder on ALL labels from the selected fold to keep mapping consistent\n",
    "    all_labels = pd.concat([y_train, y_test]).astype(str).values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_labels)\n",
    "\n",
    "    # Encode each split\n",
    "    train_labels_enc  = le.transform(train_labels)\n",
    "    train2_labels_enc = le.transform(train2_labels)\n",
    "\n",
    "    test_labels_enc   = le.transform(test_labels)\n",
    "    test2_labels_enc  = le.transform(test2_labels)\n",
    "\n",
    "    # Optional: print label→index mapping\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(\"Label mapping:\")\n",
    "    for k, v in label_mapping.items():\n",
    "        print(f\"  {k} -> {v}\")\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        OvR_classifiers,\n",
    "        train_features, train_labels_enc,\n",
    "        test_features, test_labels_enc,\n",
    "        word_file=\"OvR results Multiclass fold-\"+ str(SELECTED_FOLD)+\".docx\"\n",
    "    )\n",
    "    print(accuracies)\n",
    "\n",
    "\n",
    "    \n",
    "    # 1) Get class-probability predictions for train2 and test2\n",
    "    et_ovr = OvR_classifiers[\"ET-OvR\"]\n",
    "\n",
    "    ET_train_proba = et_ovr.predict_proba(train2_features)  # shape (n_train2, n_classes)\n",
    "    ET_test_proba  = et_ovr.predict_proba(test2_features)   # shape (n_test2,  n_classes)\n",
    "\n",
    "    # Optional sanity check\n",
    "    print(\"ET_train_proba shape:\", ET_train_proba.shape)\n",
    "    print(\"ET_test_proba shape:\",  ET_test_proba.shape)\n",
    "\n",
    "    # 2) If you only need numeric arrays for the meta-model:\n",
    "    #    Simply stack original features and probabilities horizontally.\n",
    "\n",
    "    new_ET_OvR_training_features = np.hstack([train2_features, ET_train_proba])\n",
    "    new_ET_OvR_test_features     = np.hstack([test2_features,  ET_test_proba])\n",
    "\n",
    "    print(\"new_ET_OvR_training_features shape:\", new_ET_OvR_training_features.shape)\n",
    "    print(\"new_ET_OvR_test_features shape:\",     new_ET_OvR_test_features.shape)\n",
    "\n",
    "    # 3) Train/evaluate second-stage classifiers using stacked features\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        classifiers,\n",
    "        new_ET_OvR_training_features, train2_labels_enc,\n",
    "        new_ET_OvR_test_features,     test2_labels_enc,\n",
    "        word_file=\"ET-OvR as feature extractor Multiclass results fold-\"+ str(SELECTED_FOLD)+\".docx\",\n",
    "    )\n",
    "\n",
    "    print(\"Stacked-model accuracies:\")\n",
    "    print(accuracies)\n",
    "    # 1) Get class-probability predictions for train2 and test2\n",
    "    RF_ovr = OvR_classifiers[\"RF-OvR\"]\n",
    "\n",
    "    RF_train_proba = RF_ovr.predict_proba(train2_features)  # shape (n_train2, n_classes)\n",
    "    RF_test_proba  = RF_ovr.predict_proba(test2_features)   # shape (n_test2,  n_classes)\n",
    "\n",
    "    # Optional sanity check\n",
    "    print(\"RF_train_proba shape:\", RF_train_proba.shape)\n",
    "    print(\"RF_test_proba shape:\",  RF_test_proba.shape)\n",
    "\n",
    "    # 2) If you only need numeric arrays for the meta-model:\n",
    "    #    Simply stack original features and probabilities horizontally.\n",
    "\n",
    "    new_RF_OvR_training_features = np.hstack([train2_features, RF_train_proba])\n",
    "    new_RF_OvR_test_features     = np.hstack([test2_features,  RF_test_proba])\n",
    "\n",
    "    print(\"new_RF_OvR_training_features shape:\", new_RF_OvR_training_features.shape)\n",
    "    print(\"new_RF_OvR_test_features shape:\",     new_RF_OvR_test_features.shape)\n",
    "\n",
    "    # 3) Train/evaluate second-stage classifiers using stacked features\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        classifiers,\n",
    "        new_RF_OvR_training_features, train2_labels_enc,\n",
    "        new_RF_OvR_test_features,     test2_labels_enc,\n",
    "        word_file=\"RF-OvR as feature extractor Multiclass results fold-\"+ str(SELECTED_FOLD)+\".docx\",\n",
    "    )\n",
    "\n",
    "    print(\"Stacked-model accuracies:\")\n",
    "    print(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_from_test20_v2(test20_df, ovr_classifier, final_classifier, class_names,\n",
    "                                   save_path=\"random_report.png\", dpi=300):\n",
    "    \"\"\"\n",
    "    Generate A5 visualization for a random sample from test20_df.\n",
    "    Page split into 8 slots:\n",
    "      1- Final decision + probability %\n",
    "      2- Histology image\n",
    "      3- OvR probabilities\n",
    "      4–8- GLCM feature groups\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "    plt.rcParams.update({'font.size': 7})\n",
    "    plt.rcParams.update({\n",
    "        \"axes.facecolor\": \"white\",   # background of axes\n",
    "        \"figure.facecolor\": \"white\"  # background of entire figure\n",
    "    })\n",
    "    plt.rcParams.update({\n",
    "        \"axes.grid\": True,          # turn on grid\n",
    "        \"grid.color\": \"gray\",       # gridline color\n",
    "        \"grid.linestyle\": \"--\",     # dashed\n",
    "        \"grid.linewidth\": 0.5       # thin\n",
    "    })\n",
    "    # --- Pick random row ---\n",
    "    idx = np.random.choice(test20_df.index)\n",
    "    row = test20_df.loc[idx]\n",
    "\n",
    "    # --- Extract raw GLCM features ---\n",
    "    glcm_cols = [c for c in test20_df.columns\n",
    "                 if any(k in c for k in [\"contrast\", \"energy\", \"homogeneity\", \"homogynity\", \"correlation\", \"entropy\"])]\n",
    "    glcm_features = row[glcm_cols].values.reshape(1, -1)\n",
    "\n",
    "    # --- Compute OvR probabilities ---\n",
    "    proba_array = ovr_classifier.predict_proba(glcm_features)  # shape (1, n_classes)\n",
    "    class_labels = ovr_classifier.classes_\n",
    "    proba_cols = [f\"ET_OvR_proba_{cls}\" for cls in class_labels]\n",
    "    proba_df = pd.DataFrame(proba_array, index=[idx], columns=proba_cols)\n",
    "\n",
    "    # --- Concatenate to form final input ---\n",
    "    new_features = pd.concat([pd.DataFrame(glcm_features, index=[idx], columns=glcm_cols),\n",
    "                              proba_df], axis=1)\n",
    "\n",
    "    # --- Predict with final classifier ---\n",
    "    final_proba = final_classifier.predict_proba(new_features)[0]\n",
    "    pred_idx = int(np.argmax(final_proba))\n",
    "    pred_name = class_names[pred_idx]\n",
    "    true_label = row[\"TumorSubtype\"]\n",
    "\n",
    "    # --- Visualization Layout: 4 rows × 2 columns = 8 slots ---\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(8.3, 11.6))  # A4 size; change to (5.8, 8.3) for A5\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Slot 1: Final decision text\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].text(0.5, 0.5,\n",
    "                 f\"Final Prediction: {pred_name}\\n\"\n",
    "                 f\"True Label: {true_label}\\n\"\n",
    "                 f\"Confidence: {final_proba[pred_idx]*100:.2f}%\",\n",
    "                 fontsize=14, ha=\"center\", va=\"center\", weight=\"bold\")\n",
    "\n",
    "    # Slot 2: Histology image\n",
    "    img_path = row[\"path\"].replace(\"BreaKHis_v1/\", \"E:/BreaKHis_augmentedv2/BreaKHis_v1/\")\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        axes[1].imshow(img)\n",
    "    else:\n",
    "        axes[1].imshow(np.random.rand(100, 100, 3))\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Sample Image\")\n",
    "\n",
    "    # Slot 3: OvR probabilities\n",
    "    y_pos = np.arange(len(class_names))\n",
    "    axes[2].barh(y_pos, proba_array[0], color=\"skyblue\")\n",
    "    axes[2].barh(pred_idx, proba_array[0][pred_idx], color=\"orange\")\n",
    "    axes[2].set_yticks(y_pos, class_names, fontsize=7)\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].set_xlabel(\"Probability\")\n",
    "    axes[2].set_title(\"OvR Probabilities\")    # Slots 4–8: GLCM groups (mean ± std across distances/angles per band)\n",
    "    bands = [\"cA\", \"cH1\", \"cV1\", \"cD1\", \"cH2\", \"cV2\", \"cD2\"]\n",
    "    metrics = [\n",
    "        (\"Contrast\", \"contrast\"),\n",
    "        (\"Energy\", \"energy\"),\n",
    "        (\"Homogeneity\", \"homogeneity\"),\n",
    "        (\"Correlation\", \"correlation\"),\n",
    "        (\"Entropy\", \"entropy\"),\n",
    "    ]\n",
    "\n",
    "    def band_metric_cols(band: str, metric_key: str):\n",
    "        \"\"\"Collect all columns for a band+metric.\n",
    "        Supports both:\n",
    "          - {band}_{metric}_d{d}_a{a} (e.g., cA_contrast_d1_a0)\n",
    "          - {band}_entropy (no distance/angle)\n",
    "        \"\"\"\n",
    "        if metric_key == \"entropy\":\n",
    "            return [c for c in glcm_cols if c == f\"{band}_entropy\" or c.startswith(f\"{band}_entropy_\")]\n",
    "        # typical case: cA_contrast_d1_a0, ...\n",
    "        prefix = f\"{band}_{metric_key}_\"\n",
    "        return [c for c in glcm_cols if c.startswith(prefix)]\n",
    "\n",
    "    for ax, (title, key) in zip(axes[3:], metrics):\n",
    "        means, stds = [], []\n",
    "        for band in bands:\n",
    "            cols = band_metric_cols(band, key)\n",
    "            if len(cols) == 0:\n",
    "                means.append(0.0)\n",
    "                stds.append(0.0)\n",
    "                continue\n",
    "            vals = pd.to_numeric(pd.Series([row[c] for c in cols]), errors=\"coerce\").dropna().values\n",
    "            if vals.size == 0:\n",
    "                means.append(0.0)\n",
    "                stds.append(0.0)\n",
    "                continue\n",
    "            means.append(float(np.mean(vals)))\n",
    "            stds.append(float(np.std(vals, ddof=1)) if vals.size > 1 else 0.0)\n",
    "\n",
    "        ax.barh(range(len(bands)), means, xerr=stds, color=\"steelblue\",\n",
    "                ecolor=\"black\", capsize=3)\n",
    "        ax.set_yticks(range(len(bands)), bands, fontsize=8)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Remove any unused slots (if fewer than 8 GLCM groups)\n",
    "    for ax in axes[3+len(metrics):]:\n",
    "        ax.axis(\"off\")\n",
    "    for ax in fig.axes:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(\"gray\")\n",
    "            spine.set_linewidth(0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return idx, pred_name, true_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions:\n",
    "# - You already have: test2_features (pd.DataFrame), test2_labels (pd.Series or array-like)\n",
    "# - You already have trained: ovr_classifier, final_classifier\n",
    "# - You already have the function: generate_report_from_test20_v2(...)\n",
    "# - Optional (recommended): test2_filepaths (pd.Series indexed like test2_features) with real image paths\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Build test20_df as expected by the report function\n",
    "#    - Must contain: GLCM feature columns, \"filepaths\", \"TumorSubtype\"\n",
    "# -----------------------------\n",
    "test20_df = test2_features.copy()\n",
    "\n",
    "# Ensure GLCM columns ordering matches OvR training (prevents silent column-order bugs)\n",
    "if hasattr(RF_ovr, \"feature_names_in_\"):\n",
    "    ovr_cols = list(RF_ovr.feature_names_in_)\n",
    "    missing = [c for c in ovr_cols if c not in test20_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing OvR feature columns in test2_features: {missing[:10]} ... ({len(missing)} total)\")\n",
    "    # Put OvR features first, keep any extra columns after (won't hurt)\n",
    "    extra = [c for c in test20_df.columns if c not in ovr_cols]\n",
    "    test20_df = test20_df[ovr_cols + extra]\n",
    "\n",
    "# Add true label column (string form is fine)\n",
    "test20_df[\"TumorSubtype\"] = pd.Series(test2_labels, index=test20_df.index).astype(str)\n",
    "test2_filepaths = test_path2\n",
    "# Add filepaths (use your real file path series if you have it)\n",
    "if \"filepaths\" not in test20_df.columns:\n",
    "    if \"test2_filepaths\" in globals() and isinstance(test2_filepaths, (pd.Series, pd.Index)):\n",
    "        test20_df[\"path\"] = pd.Series(test2_filepaths, index=test20_df.index).astype(str)\n",
    "        print(test20_df[\"path\"].head())\n",
    "    else:\n",
    "        # Fallback (report will show random image if cv2.imread fails)\n",
    "        test20_df[\"path\"] = \"\"\n",
    "test20_df[\"path\"] = pd.Series(test_path2, index=test20_df.index).astype(str)\n",
    "print(test20_df.head())\n",
    "# -----------------------------\n",
    "# 2) class_names for display\n",
    "#    Prefer your label encoder classes if you have them; otherwise derive from test2_labels\n",
    "# -----------------------------\n",
    "if \"class_names\" not in globals() or class_names is None or len(class_names) == 0:\n",
    "    # Safe default: sorted unique label strings from this fold\n",
    "    class_names = sorted(test20_df[\"TumorSubtype\"].unique().tolist())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Generate N random reports from test2_features\n",
    "# -----------------------------\n",
    "N_REPORTS = 50\n",
    "for i in range(N_REPORTS):\n",
    "    save_path = f\"report_test2_{i+1:02d}.png\"\n",
    "    idx, pred_name, true_label = generate_report_from_test20_v2(\n",
    "        test20_df=test20_df,\n",
    "        ovr_classifier=RF_ovr,\n",
    "        final_classifier=classifiers['Extra Trees'],\n",
    "        class_names=class_names,\n",
    "        save_path=save_path,\n",
    "        dpi=300,\n",
    "    )\n",
    "    print(f\"Saved {save_path} | idx={idx} | pred={pred_name} | true={true_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "zip_name = \"reports2.zip\"\n",
    "with zipfile.ZipFile(zip_name, \"w\") as zipf:\n",
    "    for i in range(20):\n",
    "        filename = os.path.join(\"/kaggle/working\", f\"{i}.png\")\n",
    "        if os.path.exists(filename):\n",
    "            zipf.write(filename, os.path.basename(filename))\n",
    "        else:\n",
    "            print(f\"⚠️ Missing file: {filename}\")\n",
    "\n",
    "print(f\"✅ Created {zip_name}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8901108,
     "sourceId": 14222728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
