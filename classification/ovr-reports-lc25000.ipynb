{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-28T16:44:58.008638Z",
     "iopub.status.busy": "2026-01-28T16:44:58.008275Z",
     "iopub.status.idle": "2026-01-28T16:45:02.265295Z",
     "shell.execute_reply": "2026-01-28T16:45:02.264282Z",
     "shell.execute_reply.started": "2026-01-28T16:44:58.008598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data manipulation and visualization\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "\n",
    "# Machine Learning models and preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "# Machine Learning classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# External libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tempfile\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, jaccard_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, roc_curve\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# External libraries\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:45:02.267731Z",
     "iopub.status.busy": "2026-01-28T16:45:02.266969Z",
     "iopub.status.idle": "2026-01-28T16:45:02.274077Z",
     "shell.execute_reply": "2026-01-28T16:45:02.273016Z",
     "shell.execute_reply.started": "2026-01-28T16:45:02.267693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid', palette='pastel')\n",
    "color = sns.color_palette(palette='pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:45:02.276882Z",
     "iopub.status.busy": "2026-01-28T16:45:02.276548Z",
     "iopub.status.idle": "2026-01-28T16:45:02.311990Z",
     "shell.execute_reply": "2026-01-28T16:45:02.311040Z",
     "shell.execute_reply.started": "2026-01-28T16:45:02.276849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_export(\n",
    "    classifiers: dict,\n",
    "    train_features, train_labels,\n",
    "    test_features, test_labels,\n",
    "    history_dict: dict = None,\n",
    "    word_file: str = \"results.docx\",\n",
    "    excel_file: str = \"results.xlsx\",\n",
    "    cm_npz_file: str = \"confusion_matrices.npz\",\n",
    "):    # Prepare Word documenat\n",
    "    doc = Document()  # Document(word_file) if os.path.exists(word_file) else Document()\n",
    "\n",
    "    # Prepare Excel workbook (one sheet per model + Summary)\n",
    "    import pandas as pd\n",
    "\n",
    "    def _safe_sheet_name(s: str) -> str:\n",
    "        # Excel sheet name max length is 31 and cannot contain: : \\ / ? * [ ]\n",
    "        bad = [\":\", \"\\\\\", \"/\", \"?\", \"*\", \"[\", \"]\"]\n",
    "        for ch in bad:\n",
    "            s = s.replace(ch, \"-\")\n",
    "        s = s.strip() or \"Sheet\"\n",
    "        return s[:31]\n",
    "\n",
    "    excel_writer = pd.ExcelWriter(excel_file, engine=\"openpyxl\")\n",
    "    summary_rows = []\n",
    "\n",
    "    # Store confusion matrices for NPZ export\n",
    "    cm_store = {}  # model_name -> cm (ndarray)\n",
    "\n",
    "    # If labels are one-hot/multilabel, convert them to integer labels\n",
    "    if isinstance(test_labels, np.ndarray) and test_labels.ndim > 1:\n",
    "        test_true = np.argmax(test_labels, axis=1)\n",
    "    else:\n",
    "        test_true = test_labels\n",
    "    \n",
    "\n",
    "    unique_labels = np.unique(test_true)\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    def _get_model_classes(m, fallback):\n",
    "        # Prefer model.classes_ (sklearn classifiers) to keep y_proba column order consistent.\n",
    "        cls = getattr(m, \"classes_\", None)\n",
    "        if cls is None:\n",
    "            return np.array(fallback)\n",
    "        return np.array(cls)\n",
    "    accuracies = []\n",
    "    tmp_files = []\n",
    "    for name, model in classifiers.items():\n",
    "        print(name)\n",
    "        \n",
    "        roc_data = []\n",
    "        # Train\n",
    "        fitted = model.fit(train_features, train_labels)\n",
    "\n",
    "        # Class order (important for y_proba columns)\n",
    "        model_classes = _get_model_classes(model, unique_labels)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_raw = model.predict(test_features)\n",
    "\n",
    "        # If predict returns one-hot/multilabel, convert to class indices\n",
    "        if isinstance(y_pred_raw, np.ndarray) and y_pred_raw.ndim > 1:\n",
    "            y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "        else:\n",
    "            y_pred = y_pred_raw\n",
    "        \n",
    "        # Try to get probabilities\n",
    "        try:\n",
    "            y_proba = model.predict_proba(test_features)\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "        # Confusion matrix (use model_classes for stable axis ordering)\n",
    "        cm = confusion_matrix(test_true, y_pred, labels=model_classes)\n",
    "        cm_store[name] = cm\n",
    "\n",
    "        # Specificity per class (one-vs-rest): TN / (TN + FP)\n",
    "        # For classes absent in the ground-truth of this split, specificity is not informative; skip them.\n",
    "        specs = []\n",
    "        for i in range(cm.shape[0]):\n",
    "            support_true = cm[i, :].sum()\n",
    "            if support_true == 0:\n",
    "                continue  # skip absent class in test_true\n",
    "            tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "            fp = cm[:, i].sum() - cm[i, i]\n",
    "            denom = (tn + fp)\n",
    "            specs.append(tn / denom if denom > 0 else np.nan)\n",
    "        avg_spec = float(np.nanmean(specs)) if len(specs) else np.nan\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(test_true, y_pred),\n",
    "            \"Precision\": precision_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Recall\": recall_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"F1 Score\": f1_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Jaccard Index\": jaccard_score(test_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Matthews CorrCoef\": matthews_corrcoef(test_true, y_pred),\n",
    "            \"Cohen’s Kappa\": cohen_kappa_score(test_true, y_pred),\n",
    "            \"Specificity\": avg_spec,\n",
    "        }\n",
    "\n",
    "        # AUC\n",
    "        auc = np.nan\n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                # Ensure y_proba columns align to model_classes\n",
    "                # (sklearn guarantees this ordering for predict_proba)\n",
    "                if len(model_classes) == 2:\n",
    "                    # Binary AUC: use the probability of the positive class (second column)\n",
    "                    pos_label = model_classes[1]\n",
    "                    y_true_bin = (np.array(test_true) == pos_label).astype(int)\n",
    "                    y_score = y_proba[:, 1]\n",
    "                    auc = roc_auc_score(y_true_bin, y_score)\n",
    "                    fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
    "                    roc_data.append((name, fpr, tpr, auc))\n",
    "                else:\n",
    "                    # Multiclass AUC (OvR): compute per-class AUC then macro-average over valid classes\n",
    "                    y_bin = label_binarize(test_true, classes=model_classes)\n",
    "                    auc_list = []\n",
    "                    for i, cls in enumerate(model_classes):\n",
    "                        # Skip classes with no positives or no negatives in this test split\n",
    "                        pos = int(y_bin[:, i].sum())\n",
    "                        if pos == 0 or pos == y_bin.shape[0]:\n",
    "                            continue\n",
    "                        fpr, tpr, _ = roc_curve(y_bin[:, i], y_proba[:, i])\n",
    "                        auc_i = roc_auc_score(y_bin[:, i], y_proba[:, i])\n",
    "                        auc_list.append(float(auc_i))\n",
    "                        roc_data.append((f\"Class {cls}\", fpr, tpr, auc_i))\n",
    "                    if len(auc_list):\n",
    "                        auc = float(np.mean(auc_list))\n",
    "            except Exception:\n",
    "                pass\n",
    "        metrics[\"AUC\"] = auc\n",
    "        accuracies.append((name, metrics[\"Accuracy\"] * 100))\n",
    "\n",
    "        # -------------------------\n",
    "        # Save metrics to Excel\n",
    "        # -------------------------\n",
    "        summary_rows.append({\"Model\": name, **metrics})\n",
    "\n",
    "        sheet = _safe_sheet_name(name)\n",
    "        df_metrics = pd.DataFrame({\"Metric\": list(metrics.keys()), \"Value\": list(metrics.values())})\n",
    "        df_metrics[\"Value\"] = df_metrics[\"Value\"].apply(lambda x: \"N/A\" if (isinstance(x, float) and np.isnan(x)) else x)\n",
    "        df_metrics.to_excel(excel_writer, sheet_name=sheet, index=False, startrow=0)\n",
    "\n",
    "        # Also store confusion matrix in the same sheet (below metrics)\n",
    "        df_cm = pd.DataFrame(cm)\n",
    "        cm_start_row = len(df_metrics) + 3\n",
    "        df_cm.to_excel(excel_writer, sheet_name=sheet, index=True, header=True, startrow=cm_start_row)\n",
    "\n",
    "        # Write to Word\n",
    "        doc.add_heading(f\"Evaluation: {name}\", level=1)\n",
    "        for m, v in metrics.items():\n",
    "            doc.add_paragraph(f\"{m}: {v if not np.isnan(v) else 'N/A'}\")\n",
    "\n",
    "        # Confusion matrix plot\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "            plt.title(name)\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"True\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(tmp.name, dpi=300)\n",
    "            plt.close()\n",
    "            doc.add_picture(tmp.name, width=Inches(4))\n",
    "            tmp_files.append(tmp.name)  # store path for later deletion\n",
    "\n",
    "        # Loss plot if available\n",
    "        if history_dict and name in history_dict:\n",
    "            hist = history_dict[name].history\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "                plt.figure(figsize=(6,6))\n",
    "                plt.plot(hist['loss'], label='Train Loss')\n",
    "                if 'val_loss' in hist:\n",
    "                    plt.plot(hist['val_loss'], label='Val Loss')\n",
    "                plt.title(f\"Loss: {name}\")\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(tmp.name, dpi=300)\n",
    "                plt.close()\n",
    "                doc.add_picture(tmp.name, width=Inches(6))\n",
    "                tmp_files.append(tmp.name)  # store path for later deletion\n",
    "\n",
    "        # Combined ROC for binary classifiers\n",
    "        if roc_data:\n",
    "            doc.add_heading(\"Combined ROC Curves\", level=1)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "                plt.figure(figsize=(6,6))\n",
    "                for label, fpr, tpr, auc_score in roc_data:\n",
    "                    plt.plot(fpr, tpr, label=f\"{label} (AUC={auc_score:.2f})\")\n",
    "                plt.plot([0,1],[0,1], linestyle='--', color='black')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('ROC Curves')\n",
    "                plt.legend(fontsize=8)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(tmp.name, dpi=300)\n",
    "                plt.close()\n",
    "                doc.add_picture(tmp.name, width=Inches(6))\n",
    "                tmp_files.append(tmp.name)  # store path for later deletion\n",
    "        print(metrics[\"Accuracy\"])    # Summary sheet (one row per model)\n",
    "    if summary_rows:\n",
    "        df_summary = pd.DataFrame(summary_rows)\n",
    "        # Make the sheet human-friendly (order common metrics first if present)\n",
    "        preferred = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Jaccard Index\", \"Matthews CorrCoef\", \"Cohen’s Kappa\", \"Specificity\", \"AUC\"]\n",
    "        cols = [c for c in preferred if c in df_summary.columns] + [c for c in df_summary.columns if c not in preferred]\n",
    "        df_summary = df_summary[cols]\n",
    "        df_summary.to_excel(excel_writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "    # Persist files\n",
    "    doc.save(word_file)\n",
    "    excel_writer.close()\n",
    "\n",
    "    # Save confusion matrices to NPZ\n",
    "    # - Each model is stored under its name as a key\n",
    "    # - Also store class mapping for reproducibility\n",
    "    np.savez_compressed(\n",
    "        cm_npz_file,\n",
    "        **{str(k): v for k, v in cm_store.items()},\n",
    "        classes=unique_labels,\n",
    "    )\n",
    "\n",
    "    # Delete temp files safely\n",
    "    for f in tmp_files:\n",
    "        os.unlink(f)\n",
    "    print(f\"✅ Results saved to {word_file}\")\n",
    "    print(f\"✅ Metrics saved to {excel_file}\")\n",
    "    print(f\"✅ Confusion matrices saved to {cm_npz_file}\")\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z",
     "iopub.execute_input": "2026-01-28T16:45:02.313924Z",
     "iopub.status.busy": "2026-01-28T16:45:02.313336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "FILE_PATH = \"/kaggle/input/lc25000-wt-glcm-4a4d/glcm_features_LC25000.xlsx\"\n",
    "PATH_COL  = \"path\"   # change if your column name differs\n",
    "SEED      = 42\n",
    "N_SPLITS  = 5\n",
    "\n",
    "# -----------------------------\n",
    "# Load (xlsx)\n",
    "# -----------------------------\n",
    "df = pd.read_excel(FILE_PATH).copy()\n",
    "\n",
    "if PATH_COL not in df.columns:\n",
    "    raise ValueError(f\"PATH_COL='{PATH_COL}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Label = last folder in path\n",
    "# (parent folder of the image file)\n",
    "# -----------------------------\n",
    "p = df[PATH_COL].astype(str).str.replace(\"\\\\\", \"/\", regex=False).str.rstrip(\"/\")\n",
    "df[\"label\"] = p.str.split(\"/\").str[-2]   # folder before filename\n",
    "\n",
    "# Drop invalid rows (optional but recommended)\n",
    "df = df[df[\"label\"].notna() & (df[\"label\"].str.len() > 0)].reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Stratified 5-fold\n",
    "# -----------------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "df[\"fold\"] = -1\n",
    "\n",
    "X_dummy = df[[PATH_COL]]         # placeholder\n",
    "y = df[\"label\"].astype(str)\n",
    "\n",
    "for fold_id, (_, val_idx) in enumerate(skf.split(X_dummy, y), start=1):\n",
    "    df.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "# -----------------------------\n",
    "# Checks\n",
    "# -----------------------------\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Fold counts:\\n\", df[\"fold\"].value_counts().sort_index())\n",
    "print(\"\\nLabel distribution per fold:\\n\", pd.crosstab(df[\"fold\"], df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "et_base = ExtraTreesClassifier(\n",
    "    n_estimators=800,          # more trees for stability\n",
    "    max_depth=None,           # let trees grow full\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",      # random subspace often helps with lots of features\n",
    "    random_state=42\n",
    ")\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=800,          # more trees for stability\n",
    "    max_depth=None,           # let trees grow full\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",      # random subspace often helps with lots of features\n",
    "    random_state=42\n",
    ")\n",
    "OvR_classifiers = {\n",
    "    # \"ET-OvR\": OneVsRestClassifier(et_base, n_jobs=-1),\n",
    "    \"RF-OvR\": OneVsRestClassifier(rf_base, n_jobs=-1)\n",
    "}\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42,class_weight=\"balanced\"),\n",
    "    \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df.columns if c not in {PATH_COL, \"label\", \"fold\"}]\n",
    "\n",
    "# -----------------------------\n",
    "# Select fold\n",
    "# -----------------------------\n",
    "\n",
    "for SELECTED_FOLD in [1]:\n",
    "    train_df = df[df[\"fold\"] != SELECTED_FOLD].reset_index(drop=True)\n",
    "    test_df  = df[df[\"fold\"] == SELECTED_FOLD].reset_index(drop=True)\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"label\"]\n",
    "    X_train_path = train_df[\"path\"]\n",
    "    \n",
    "    X_test  = test_df[feature_cols]\n",
    "    y_test  = test_df[\"label\"]\n",
    "    X_test_path = test_df[\"path\"]\n",
    "    \n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final sanity checks\n",
    "    # -----------------------------\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_test) == len(y_test)\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape, \"Labels:\", y_train.nunique())\n",
    "    print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # =========================================\n",
    "    # 4) STRATIFIED 50:50 SPLIT FOR TRAIN AND TEST\n",
    "    # =========================================\n",
    "\n",
    "    sss_train = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    sss_test  = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "    # ---- Stratified 50:50 split for TRAIN ----\n",
    "    for idx1, idx2 in sss_train.split(X_train, y_train):\n",
    "        train_features  = X_train.iloc[idx1]\n",
    "        train_labels    = y_train.iloc[idx1]\n",
    "        train_path      = X_train_path.iloc[idx1]\n",
    "\n",
    "        train2_features = X_train.iloc[idx2]\n",
    "        train2_labels   = y_train.iloc[idx2]\n",
    "        train_path2      = X_train_path.iloc[idx2]\n",
    "\n",
    "    # ---- Stratified 50:50 split for TEST ----\n",
    "    for idx1, idx2 in sss_test.split(X_test, y_test):\n",
    "        test_features   = X_test.iloc[idx1]\n",
    "        test_labels     = y_test.iloc[idx1]\n",
    "        test_path       = X_test_path[idx1]\n",
    "\n",
    "        test2_features  = X_test.iloc[idx2]\n",
    "        test2_labels    = y_test.iloc[idx2]\n",
    "        test_path2      = X_test_path[idx2]\n",
    "\n",
    "\n",
    "    print(\"train_features:\", train_features.shape, \"train2_features:\", train2_features.shape)\n",
    "    print(\"test_features:\",  test_features.shape,  \"test2_features:\",  test2_features.shape)\n",
    "\n",
    "\n",
    "    # =========================================\n",
    "    # 5) ENCODE LABELS INTO NUMERICAL FORM (FOR ML MODELS)\n",
    "    # =========================================\n",
    "\n",
    "    # Fit encoder on ALL labels from the selected fold to keep mapping consistent\n",
    "    all_labels = pd.concat([y_train, y_test]).astype(str).values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_labels)\n",
    "\n",
    "    # Encode each split\n",
    "    train_labels_enc  = le.transform(train_labels)\n",
    "    train2_labels_enc = le.transform(train2_labels)\n",
    "\n",
    "    test_labels_enc   = le.transform(test_labels)\n",
    "    test2_labels_enc  = le.transform(test2_labels)\n",
    "\n",
    "    # Optional: print label→index mapping\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(\"Label mapping:\")\n",
    "    for k, v in label_mapping.items():\n",
    "        print(f\"  {k} -> {v}\")\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        OvR_classifiers,\n",
    "        train_features, train_labels_enc,\n",
    "        test_features, test_labels_enc,\n",
    "        word_file=\"OvR results Multiclass fold-\"+ str(SELECTED_FOLD)+\".docx\"\n",
    "    )\n",
    "    print(accuracies)\n",
    "\n",
    "\n",
    "    \n",
    "    # 1) Get class-probability predictions for train2 and test2\n",
    "    et_ovr = OvR_classifiers[\"ET-OvR\"]\n",
    "\n",
    "    ET_train_proba = et_ovr.predict_proba(train2_features)  # shape (n_train2, n_classes)\n",
    "    ET_test_proba  = et_ovr.predict_proba(test2_features)   # shape (n_test2,  n_classes)\n",
    "\n",
    "    # Optional sanity check\n",
    "    print(\"ET_train_proba shape:\", ET_train_proba.shape)\n",
    "    print(\"ET_test_proba shape:\",  ET_test_proba.shape)\n",
    "\n",
    "    # 2) If you only need numeric arrays for the meta-model:\n",
    "    #    Simply stack original features and probabilities horizontally.\n",
    "\n",
    "    new_ET_OvR_training_features = np.hstack([train2_features, ET_train_proba])\n",
    "    new_ET_OvR_test_features     = np.hstack([test2_features,  ET_test_proba])\n",
    "\n",
    "    print(\"new_ET_OvR_training_features shape:\", new_ET_OvR_training_features.shape)\n",
    "    print(\"new_ET_OvR_test_features shape:\",     new_ET_OvR_test_features.shape)\n",
    "\n",
    "    # 3) Train/evaluate second-stage classifiers using stacked features\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        classifiers,\n",
    "        new_ET_OvR_training_features, train2_labels_enc,\n",
    "        new_ET_OvR_test_features,     test2_labels_enc,\n",
    "        word_file=\"ET-OvR as feature extractor Multiclass results fold-\"+ str(SELECTED_FOLD)+\".docx\",\n",
    "    )\n",
    "\n",
    "    print(\"Stacked-model accuracies:\")\n",
    "    print(accuracies)\n",
    "    # 1) Get class-probability predictions for train2 and test2\n",
    "    RF_ovr = OvR_classifiers[\"RF-OvR\"]\n",
    "\n",
    "    RF_train_proba = RF_ovr.predict_proba(train2_features)  # shape (n_train2, n_classes)\n",
    "    RF_test_proba  = RF_ovr.predict_proba(test2_features)   # shape (n_test2,  n_classes)\n",
    "\n",
    "    # Optional sanity check\n",
    "    print(\"RF_train_proba shape:\", RF_train_proba.shape)\n",
    "    print(\"RF_test_proba shape:\",  RF_test_proba.shape)\n",
    "\n",
    "    # 2) If you only need numeric arrays for the meta-model:\n",
    "    #    Simply stack original features and probabilities horizontally.\n",
    "\n",
    "    new_RF_OvR_training_features = np.hstack([train2_features, RF_train_proba])\n",
    "    new_RF_OvR_test_features     = np.hstack([test2_features,  RF_test_proba])\n",
    "\n",
    "    print(\"new_RF_OvR_training_features shape:\", new_RF_OvR_training_features.shape)\n",
    "    print(\"new_RF_OvR_test_features shape:\",     new_RF_OvR_test_features.shape)\n",
    "\n",
    "    # 3) Train/evaluate second-stage classifiers using stacked features\n",
    "\n",
    "    accuracies = train_and_evaluate_export(\n",
    "        classifiers,\n",
    "        new_RF_OvR_training_features, train2_labels_enc,\n",
    "        new_RF_OvR_test_features,     test2_labels_enc,\n",
    "        word_file=\"RF-OvR as feature extractor Multiclass results fold-\"+ str(SELECTED_FOLD)+\".docx\",\n",
    "    )\n",
    "\n",
    "    print(\"Stacked-model accuracies:\")\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_report_from_test20_v2(test20_df, ovr_classifier, final_classifier, class_names,\n",
    "                                   save_path=\"random_report.png\", dpi=300):\n",
    "    \"\"\"\n",
    "    Generate A5 visualization for a random sample from test20_df.\n",
    "    Page split into 8 slots:\n",
    "      1- Final decision + probability %\n",
    "      2- Histology image\n",
    "      3- OvR probabilities\n",
    "      4–8- GLCM feature groups\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "    plt.rcParams.update({'font.size': 7})\n",
    "    plt.rcParams.update({\n",
    "        \"axes.facecolor\": \"white\",   # background of axes\n",
    "        \"figure.facecolor\": \"white\"  # background of entire figure\n",
    "    })\n",
    "    plt.rcParams.update({\n",
    "        \"axes.grid\": True,          # turn on grid\n",
    "        \"grid.color\": \"gray\",       # gridline color\n",
    "        \"grid.linestyle\": \"--\",     # dashed\n",
    "        \"grid.linewidth\": 0.5       # thin\n",
    "    })\n",
    "    # --- Pick random row ---\n",
    "    idx = np.random.choice(test20_df.index)\n",
    "    row = test20_df.loc[idx]\n",
    "\n",
    "    # --- Extract raw GLCM features ---\n",
    "    glcm_cols = [c for c in test20_df.columns\n",
    "                 if any(k in c for k in [\"contrast\", \"energy\", \"homogeneity\", \"homogynity\", \"correlation\", \"entropy\"])]\n",
    "    glcm_series = row[glcm_cols]\n",
    "\n",
    "    # convert everything to numeric; non-numeric becomes NaN\n",
    "    glcm_series = pd.to_numeric(glcm_series, errors=\"coerce\")\n",
    "    \n",
    "    # handle NaN/inf (choose one strategy)\n",
    "    glcm_series = glcm_series.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    glcm_features = glcm_series.to_numpy(dtype=np.float64).reshape(1, -1)\n",
    "\n",
    "    # --- Compute OvR probabilities ---\n",
    "    proba_array = ovr_classifier.predict_proba(glcm_features)  # shape (1, n_classes)\n",
    "    class_labels = ovr_classifier.classes_\n",
    "    proba_cols = [f\"ET_OvR_proba_{cls}\" for cls in class_labels]\n",
    "    proba_df = pd.DataFrame(proba_array, index=[idx], columns=proba_cols)\n",
    "\n",
    "    # --- Concatenate to form final input ---\n",
    "    new_features = pd.concat([pd.DataFrame(glcm_features, index=[idx], columns=glcm_cols),\n",
    "                              proba_df], axis=1)\n",
    "\n",
    "    # --- Predict with final classifier ---\n",
    "    final_proba = final_classifier.predict_proba(new_features)[0]\n",
    "    pred_idx = int(np.argmax(final_proba))\n",
    "    pred_name = class_names[pred_idx]\n",
    "    true_label = row[\"TumorSubtype\"]\n",
    "\n",
    "    # --- Visualization Layout: 4 rows × 2 columns = 8 slots ---\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(8.3, 11.6))  # A4 size; change to (5.8, 8.3) for A5\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Slot 1: Final decision text\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].text(0.5, 0.5,\n",
    "                 f\"Final Prediction: {pred_name}\\n\"\n",
    "                 f\"True Label: {true_label}\\n\"\n",
    "                 f\"Confidence: {final_proba[pred_idx]*100:.2f}%\",\n",
    "                 fontsize=14, ha=\"center\", va=\"center\", weight=\"bold\")\n",
    "\n",
    "    # Slot 2: Histology image\n",
    "    img_path = row[\"path\"]\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        axes[1].imshow(img)\n",
    "    else:\n",
    "        axes[1].imshow(np.random.rand(100, 100, 3))\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Sample Image\")\n",
    "\n",
    "    # Slot 3: OvR probabilities\n",
    "    y_pos = np.arange(len(class_names))\n",
    "    axes[2].barh(y_pos, proba_array[0], color=\"skyblue\")\n",
    "    axes[2].barh(pred_idx, proba_array[0][pred_idx], color=\"orange\")\n",
    "    axes[2].set_yticks(y_pos, class_names, fontsize=7)\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].set_xlabel(\"Probability\")\n",
    "    axes[2].set_title(\"OvR Probabilities\")    # Slots 4–8: GLCM groups (mean ± std across distances/angles per band)\n",
    "    bands = [\"cA\", \"cH1\", \"cV1\", \"cD1\", \"cH2\", \"cV2\", \"cD2\"]\n",
    "    metrics = [\n",
    "        (\"Contrast\", \"contrast\"),\n",
    "        (\"Energy\", \"energy\"),\n",
    "        (\"Homogeneity\", \"homogeneity\"),\n",
    "        (\"Correlation\", \"correlation\"),\n",
    "        (\"Entropy\", \"entropy\"),\n",
    "    ]\n",
    "\n",
    "    def band_metric_cols(band: str, metric_key: str):\n",
    "        \"\"\"Collect all columns for a band+metric.\n",
    "        Supports both:\n",
    "          - {band}_{metric}_d{d}_a{a} (e.g., cA_contrast_d1_a0)\n",
    "          - {band}_entropy (no distance/angle)\n",
    "        \"\"\"\n",
    "        if metric_key == \"entropy\":\n",
    "            return [c for c in glcm_cols if c == f\"{band}_entropy\" or c.startswith(f\"{band}_entropy_\")]\n",
    "        # typical case: cA_contrast_d1_a0, ...\n",
    "        prefix = f\"{band}_{metric_key}_\"\n",
    "        return [c for c in glcm_cols if c.startswith(prefix)]\n",
    "\n",
    "    for ax, (title, key) in zip(axes[3:], metrics):\n",
    "        means, stds = [], []\n",
    "        for band in bands:\n",
    "            cols = band_metric_cols(band, key)\n",
    "            if len(cols) == 0:\n",
    "                means.append(0.0)\n",
    "                stds.append(0.0)\n",
    "                continue\n",
    "            vals = pd.to_numeric(pd.Series([row[c] for c in cols]), errors=\"coerce\").dropna().values\n",
    "            if vals.size == 0:\n",
    "                means.append(0.0)\n",
    "                stds.append(0.0)\n",
    "                continue\n",
    "            means.append(float(np.mean(vals)))\n",
    "            stds.append(float(np.std(vals, ddof=1)) if vals.size > 1 else 0.0)\n",
    "\n",
    "        ax.barh(range(len(bands)), means, xerr=stds, color=\"steelblue\",\n",
    "                ecolor=\"black\", capsize=3)\n",
    "        ax.set_yticks(range(len(bands)), bands, fontsize=8)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Remove any unused slots (if fewer than 8 GLCM groups)\n",
    "    for ax in axes[3+len(metrics):]:\n",
    "        ax.axis(\"off\")\n",
    "    for ax in fig.axes:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(\"gray\")\n",
    "            spine.set_linewidth(0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return idx, pred_name, true_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Build test20_df as expected by the report function\n",
    "#    - Must contain: GLCM feature columns, \"filepaths\", \"TumorSubtype\"\n",
    "# -----------------------------\n",
    "test20_df = test2_features.copy()\n",
    "\n",
    "# Ensure GLCM columns ordering matches OvR training (prevents silent column-order bugs)\n",
    "if hasattr(RF_ovr, \"feature_names_in_\"):\n",
    "    ovr_cols = list(RF_ovr.feature_names_in_)\n",
    "    missing = [c for c in ovr_cols if c not in test20_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing OvR feature columns in test2_features: {missing[:10]} ... ({len(missing)} total)\")\n",
    "    # Put OvR features first, keep any extra columns after (won't hurt)\n",
    "    extra = [c for c in test20_df.columns if c not in ovr_cols]\n",
    "    test20_df = test20_df[ovr_cols + extra]\n",
    "\n",
    "# Add true label column (string form is fine)\n",
    "test20_df[\"TumorSubtype\"] = pd.Series(test2_labels, index=test20_df.index).astype(str)\n",
    "test2_filepaths = test_path2\n",
    "# Add filepaths (use your real file path series if you have it)\n",
    "if \"filepaths\" not in test20_df.columns:\n",
    "    if \"test2_filepaths\" in globals() and isinstance(test2_filepaths, (pd.Series, pd.Index)):\n",
    "        test20_df[\"path\"] = pd.Series(test2_filepaths, index=test20_df.index).astype(str)\n",
    "        print(test20_df[\"path\"].head())\n",
    "    else:\n",
    "        # Fallback (report will show random image if cv2.imread fails)\n",
    "        test20_df[\"path\"] = \"\"\n",
    "test20_df[\"path\"] = pd.Series(test_path2, index=test20_df.index).astype(str)\n",
    "print(test20_df.head())\n",
    "# -----------------------------\n",
    "# 2) class_names for display\n",
    "#    Prefer your label encoder classes if you have them; otherwise derive from test2_labels\n",
    "# -----------------------------\n",
    "if \"class_names\" not in globals() or class_names is None or len(class_names) == 0:\n",
    "    # Safe default: sorted unique label strings from this fold\n",
    "    class_names = sorted(test20_df[\"TumorSubtype\"].unique().tolist())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Generate N random reports from test2_features\n",
    "# -----------------------------\n",
    "N_REPORTS = 50\n",
    "for i in range(N_REPORTS):\n",
    "    save_path = f\"report_test2_{i+1:02d}.png\"\n",
    "    idx, pred_name, true_label = generate_report_from_test20_v2(\n",
    "        test20_df=test20_df,\n",
    "        ovr_classifier=RF_ovr,\n",
    "        final_classifier=classifiers['LightGBM'],\n",
    "        class_names=class_names,\n",
    "        save_path=save_path,\n",
    "        dpi=300,\n",
    "    )\n",
    "    print(f\"Saved {save_path} | idx={idx} | pred={pred_name} | true={true_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-28T16:46:03.369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "\n",
    "# --- TIF files ---\n",
    "with zipfile.ZipFile(\"ovr reports on lc25000.zip\", 'w') as zf:\n",
    "    for f in glob.glob(\"*.png\"):\n",
    "        zf.write(f, f.split(\"/\")[-1])\n",
    "print(\"LC25000 Report files.zip\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 601280,
     "sourceId": 1079953,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9265058,
     "sourceId": 14506077,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
