{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c610c371-164c-48be-bfb7-18d49364ef17",
    "_uuid": "e9553da4-50d4-4400-86a1-9e17015fb582",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Breast Cancer modules using ML\n",
    "**By: Mustafa Adil**\n",
    "\n",
    "\n",
    "in this project will use augmented breakHIS dataset 51.2k images overall balanced with 1600 image per subtype of the 8 subtypes in each zoom of the 4 zoom parameters. Only th 40X zoom will be used in this code so the number of images will be 6400 images for each malignant and benign type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2e7909-e8d2-4e6d-9938-f278af5968ff",
    "_uuid": "0549a974-10f7-433e-8024-f55c5c4f89a3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# 1. Define Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a11ab3f3-515d-4bfe-a302-33d77a197bb3",
    "_uuid": "3c3d957f-84f2-4cd1-bca0-124b157796cc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T20:39:56.698729Z",
     "iopub.status.busy": "2025-12-01T20:39:56.698375Z",
     "iopub.status.idle": "2025-12-01T20:40:34.805836Z",
     "shell.execute_reply": "2025-12-01T20:40:34.804791Z",
     "shell.execute_reply.started": "2025-12-01T20:39:56.698696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "import tempfile\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "# Machine Learning models and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Machine Learning classifiers\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier,\n",
    "    ExtraTreesClassifier, BaggingClassifier, StackingClassifier,\n",
    "    VotingClassifier, HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier,\n",
    "    Perceptron\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# External libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# ML Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, jaccard_score, matthews_corrcoef,\n",
    "    cohen_kappa_score, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# Deep Learning (TensorFlow/Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, \n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "# Word Document handling\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "# IPython display utilities\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7347b68c-f5a3-45ef-81f4-ec04a3f5242a",
    "_uuid": "459828a0-423d-4437-a9a2-58cb39d62fcf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-01T20:40:34.807755Z",
     "iopub.status.busy": "2025-12-01T20:40:34.807041Z",
     "iopub.status.idle": "2025-12-01T20:40:34.813740Z",
     "shell.execute_reply": "2025-12-01T20:40:34.812251Z",
     "shell.execute_reply.started": "2025-12-01T20:40:34.807732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid', palette='pastel')\n",
    "color = sns.color_palette(palette='pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0edf4ee-1b0c-49f1-9b99-f1f18965675b",
    "_uuid": "af66b0dc-9ce5-495b-86aa-70f74cc8d616",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# 2. Load images from the dataset\n",
    "in this part will do the following\n",
    "* only the 40X zoom will be selected\n",
    "* each file path will be stored in the variable ***filepaths*** and the crosponding label in the variable ***labels*** in the same index.\n",
    "* create a data frame ***df*** to work like excel sheet with columnes filepath and label.\n",
    "* show sample of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T20:45:58.412971Z",
     "iopub.status.busy": "2025-12-01T20:45:58.411918Z",
     "iopub.status.idle": "2025-12-01T20:45:58.424464Z",
     "shell.execute_reply": "2025-12-01T20:45:58.423453Z",
     "shell.execute_reply.started": "2025-12-01T20:45:58.412939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def compute_glcm_features(image):\n",
    "    distances = [1, 2, 3, 4]  # You can experiment with different distances\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # You can experiment with different angles\n",
    "    \n",
    "    # Apply 2D Discrete Wavelet Transform (DWT)\n",
    "    coeffs = pywt.wavedec2(image, 'db1', level=2)  # Decomposition up to 2 levels\n",
    "    cA, (cH1, cV1, cD1), (cH2, cV2, cD2) = coeffs  # Extract subbands\n",
    "    \n",
    "    # Function to compute GLCM and extract features\n",
    "    def extract_glcm_features(subband):\n",
    "        glcm = graycomatrix(subband, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "        contrast = graycoprops(glcm, 'contrast').ravel()\n",
    "        energy = graycoprops(glcm, 'energy').ravel()\n",
    "        homogeneity = graycoprops(glcm, 'homogeneity').ravel()\n",
    "        correlation = graycoprops(glcm, 'correlation').ravel()\n",
    "        entropy = -np.sum(glcm * np.log(glcm + 1e-10))  # Compute entropy manually\n",
    "        return np.concatenate((contrast, energy, homogeneity, correlation, [entropy]))\n",
    "    \n",
    "    # Extract GLCM features for each wavelet subband\n",
    "    features = []\n",
    "    for subband in [cA, cH1, cV1, cD1, cH2, cV2, cD2]:\n",
    "        features.append(extract_glcm_features(subband.astype(np.uint8)))  # Convert float to uint8 for GLCM\n",
    "    \n",
    "    return np.concatenate(features)  # Concatenate all extracted features\n",
    "\n",
    "\n",
    "def process_one_image(path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    return compute_glcm_features(image)\n",
    "    \n",
    "def load_and_compute_glcm_features(paths):\n",
    "    features = []\n",
    "    total = len(paths)\n",
    "    for i, path in enumerate(tqdm(paths, desc=\"Processing Images\", unit=\"img\")):\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        dwt_features = compute_glcm_features(image)\n",
    "        features.append(dwt_features)\n",
    "    return np.array(features)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def par_load_and_compute_glcm_features(paths, n_jobs=-1):\n",
    "    # n_jobs=-1 uses all available CPU cores\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_one_image)(p) for p in tqdm(paths, desc=\"Processing Images\", unit=\"img\")\n",
    "    )\n",
    "    return np.vstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T20:46:05.242830Z",
     "iopub.status.busy": "2025-12-01T20:46:05.242466Z",
     "iopub.status.idle": "2025-12-01T21:34:08.642514Z",
     "shell.execute_reply": "2025-12-01T21:34:08.641445Z",
     "shell.execute_reply.started": "2025-12-01T20:46:05.242804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "csv_path  = \"E:/BreaKHis_augmentedv2/aug_manifest_filled.csv\"\n",
    "base_dir  = \"E:/BreaKHis_augmentedv2\"\n",
    "\n",
    "# 1) Read the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head())\n",
    "# 2) Keep only fold 1\n",
    "#   Change 'fold' to the actual column name if it is different (e.g., 'Fold', 'kfold')\n",
    "df_fold1 = df[df[\"fold\"] == 1].copy()\n",
    "\n",
    "# 3) Prepend base_dir to the 'filename' column\n",
    "df_fold1[\"filename\"] = df_fold1[\"filename\"].apply(\n",
    "    lambda x: os.path.join(base_dir, str(x).lstrip(\"/\"))\n",
    ")\n",
    "# 4) Prepare paths and labels\n",
    "paths  = df_fold1[\"filename\"].tolist()\n",
    "\n",
    "#   Adjust the label column name if needed (e.g., 'class', 'target')\n",
    "# labels = df_fold1[\"label\"].to_numpy()\n",
    "\n",
    "# 5) Compute GLCM features\n",
    "features = par_load_and_compute_glcm_features(paths)\n",
    "\n",
    "# 'features' corresponds to GLCM features per image in 'paths'\n",
    "# 'labels' are the ground-truth labels for those images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T21:36:22.973810Z",
     "iopub.status.busy": "2025-12-01T21:36:22.973411Z",
     "iopub.status.idle": "2025-12-01T21:37:33.091551Z",
     "shell.execute_reply": "2025-12-01T21:37:33.090574Z",
     "shell.execute_reply.started": "2025-12-01T21:36:22.973783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "distances = [1, 2, 3, 4]\n",
    "angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "angle_labels = [0, 45, 90, 135]   # for readable names\n",
    "\n",
    "subbands = [\"cA\", \"cH1\", \"cV1\", \"cD1\", \"cH2\", \"cV2\", \"cD2\"]\n",
    "props = [\"contrast\", \"energy\", \"homogeneity\", \"correlation\"]\n",
    "\n",
    "feature_cols = []\n",
    "\n",
    "for sb in subbands:\n",
    "    for prop in props:\n",
    "        # graycoprops output is shape (len(distances), len(angles))\n",
    "        # .ravel() flattens with distance index first, then angle index\n",
    "        for d in distances:\n",
    "            for ang_deg in angle_labels:\n",
    "                feature_cols.append(f\"{sb}_{prop}_d{d}_a{ang_deg}\")\n",
    "    # one entropy per subband\n",
    "    feature_cols.append(f\"{sb}_entropy\")\n",
    "\n",
    "len(feature_cols)  # should be 455\n",
    "\n",
    "df_features = pd.DataFrame(features, columns=feature_cols)\n",
    "df = pd.concat(\n",
    "    [pd.Series(paths, name=\"path\"), df_features],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df.to_csv(\"glcm_features_fold1.csv\", index=False)\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(\"glcm_features_fold1.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 209316,
     "sourceId": 999617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
